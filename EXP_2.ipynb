{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f89ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "135d955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "NUM_LABELS=2\n",
    "keep_cols=['text','labels']\n",
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "model_name = 'distilBERT'\n",
    "training_data='ISOT'\n",
    "text_col='text'\n",
    "max_length = 512\n",
    "batch_size = 8 \n",
    "num_train_epochs=3 \n",
    "n_training = 2000\n",
    "seed=101\n",
    "experiment = 2\n",
    "output_dir = os.path.join(os.getcwd(), f'models/{model_name}/EXP_{experiment}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee74229",
   "metadata": {},
   "source": [
    "### LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d58986e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True examples: 21417\n",
      "Number of Fake examples: 23481\n",
      "Columns:['title', 'text', 'subject', 'date']\n",
      "Most repeated subjects:\n",
      "     True:  ['politicsNews', 'worldnews']\n",
      "     False: ['News', 'politics', 'left-news', 'Government News', 'US_News', 'Middle-east']\n",
      "Average number of words in titles:\n",
      "      True:  65,\n",
      "      Fake:  94\n",
      "Average number of words in texts: \n",
      "      True:  2383,\n",
      "      Fake:  2547\n",
      "\n",
      "Printing some examples:\n",
      "\n",
      "Label: False\n",
      "Title:  Donald Trump Sends Out Embarrassing New Year’s Eve Message; This is Disturbing\n",
      "Text: Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former rea...\n",
      "Subject: News\n",
      "date: December 31, 2017\n",
      "\n",
      "Label: True\n",
      "Title: As U.S. budget fight looms, Republicans flip their fiscal script\n",
      "Text: WASHINGTON (Reuters) - The head of a conservative Republican faction in the U.S. Congress, who voted this month for a huge expansion of the national debt to pay for tax cuts, called himself a “fiscal ...\n",
      "Subject: politicsNews\n",
      "date: December 31, 2017 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cargamos los datos\n",
    "true_path = os.path.join(os.getcwd(), f'data/{training_data}/True.csv')\n",
    "fake_path = os.path.join(os.getcwd(), f'data/{training_data}/Fake.csv')\n",
    "\n",
    "true_dataset = pd.read_csv(true_path)\n",
    "fake_dataset = pd.read_csv(fake_path)\n",
    "print(f'Number of True examples: {true_dataset.shape[0]}')\n",
    "print(f'Number of Fake examples: {fake_dataset.shape[0]}')\n",
    "print(f'Columns:{true_dataset.columns.to_list()}')\n",
    "print(f'Most repeated subjects:\\n \\\n",
    "    True:  {true_dataset.subject.value_counts().index.to_list()[:10]}\\n\\\n",
    "     False: {fake_dataset.subject.value_counts().index.to_list()[:10]}')\n",
    "print(f'Average number of words in titles:\\n\\\n",
    "      True:  {round(true_dataset.title.apply(lambda x: len(x)).mean())},\\n\\\n",
    "      Fake:  {round(fake_dataset.title.apply(lambda x: len(x)).mean())}')\n",
    "print(f'Average number of words in texts: \\n\\\n",
    "      True:  {round(true_dataset.text.apply(lambda x: len(x)).mean())},\\n\\\n",
    "      Fake:  {round(fake_dataset.text.apply(lambda x: len(x)).mean())}')\n",
    "\n",
    "def print_row(input_df: pd.DataFrame, index: int, label: int) -> None:\n",
    "    if label == 0:\n",
    "        print(f\"Label: True\")\n",
    "    else:\n",
    "        print(f\"Label: False\")\n",
    "    print(f\"Title: {input_df.iat[index, 0]}\")\n",
    "    print(f\"Text: {input_df.iat[index, 1][:200]}...\")\n",
    "    print(f\"Subject: {input_df.iat[index, 2]}\")\n",
    "    print(f\"date: {input_df.iat[index, 3]}\\n\")\n",
    "\n",
    "print('\\nPrinting some examples:\\n')\n",
    "print_row(input_df=fake_dataset, index=0, label=1)\n",
    "print_row(input_df=true_dataset, index=0, label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d767ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text column word count description:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    44898.000000\n",
       "mean      2469.109693\n",
       "std       2171.617091\n",
       "min          1.000000\n",
       "25%       1234.000000\n",
       "50%       2186.000000\n",
       "75%       3105.000000\n",
       "max      51794.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_data(fake, true, text_col=text_col, random_state=seed):\n",
    "    #Añadimos la columna label\n",
    "    true['labels'] = 0\n",
    "    fake['labels'] = 1\n",
    "    data = pd.concat([true, fake], axis=0).sample(frac=1, random_state=random_state).reset_index(drop=True) #shuffle\n",
    "    #Pasamos los textos a minúscula:\n",
    "    data[text_col] = data[text_col].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    data = data[[text_col,'labels']].rename(columns={text_col:'text'})\n",
    "    return data\n",
    "\n",
    "data = preprocess_data(fake_dataset, true_dataset)\n",
    "print(f'{text_col} column word count description:')\n",
    "data.text.apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5541acf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de datos de entrenamiento: 35918. Numero de datos de test: 8980\n",
      "Train data label count:\n",
      " 0:17128,    1:18790\n",
      "Test data label count:\n",
      " 0: 4289,     1:4691\n"
     ]
    }
   ],
   "source": [
    "#Separamos en entrenamiento y test\n",
    "\n",
    "def split_data(data, test_size=0.2, random_state=seed):\n",
    "    X = data['text']\n",
    "    y = data['labels']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = test_size, random_state = random_state)\n",
    "    train_dataset = pd.concat([X_train,y_train], axis=1).reset_index(drop=True)\n",
    "    test_dataset = pd.concat([X_test,y_test], axis=1).reset_index(drop=True)\n",
    "    print(\"Numero de datos de entrenamiento: {}. Numero de datos de test: {}\".format(len(train_dataset), len(test_dataset)))\n",
    "    print(f'Train data label count:\\n 0:{train_dataset[train_dataset.labels==0].shape[0]},\\\n",
    "    1:{train_dataset[train_dataset.labels==1].shape[0]}')\n",
    "    print(f'Test data label count:\\n 0: {test_dataset[test_dataset.labels==0].shape[0]}, \\\n",
    "    1:{test_dataset[test_dataset.labels==1].shape[0]}')\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_data, test_data = split_data(data, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed2df463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de datos de entrenamiento: 2000. Numero de datos de test: 8980\n",
      "Train data label count:\n",
      " 0:968,    1:1032\n",
      "Test data label count:\n",
      " 0: 4289,     1:4691\n"
     ]
    }
   ],
   "source": [
    "def get_sample_data(data, n = data.shape[0], label_dict = {0:1, 1:1} ,random_state=seed):\n",
    "    df = pd.DataFrame(columns=data.columns)\n",
    "    for label,frac in label_dict.items():\n",
    "        tmp = data[data.labels == label].sample(frac=frac, random_state=random_state)\n",
    "        df = pd.concat([df,tmp])\n",
    "    return df.sample(frac=1, random_state=random_state)[:n].reset_index(drop=True)\n",
    "\n",
    "#train_dataset = get_sample_data(train_data, n_training, label_dict = {0:0.5, 1:1}) \n",
    "train_dataset = get_sample_data(train_data, n_training) \n",
    "test_dataset = get_sample_data(test_data)\n",
    "print(\"Numero de datos de entrenamiento: {}. Numero de datos de test: {}\".format(len(train_dataset), len(test_dataset)))\n",
    "print(f'Train data label count:\\n 0:{train_dataset[train_dataset.labels==0].shape[0]},\\\n",
    "    1:{train_dataset[train_dataset.labels==1].shape[0]}')\n",
    "print(f'Test data label count:\\n 0: {test_dataset[test_dataset.labels==0].shape[0]}, \\\n",
    "    1:{test_dataset[test_dataset.labels==1].shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b02dd8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, checkpoint, num_labels):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n",
    "        self.trainer = None\n",
    "        self.training_args = None\n",
    "    \n",
    "    def tokenize(self, data):\n",
    "        return self.tokenizer(data['text'], truncation=True)\n",
    "    \n",
    "    def compute_metrics(self, eval_pred):\n",
    "        metric = load_metric(\"accuracy\")\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    def train(self):\n",
    "        print('Empezando entrenamiento:')\n",
    "        self.trainer.train()\n",
    "        print('Fin del Entrenamiento')\n",
    "        \n",
    "    def save(self, output_dir):\n",
    "        print('Guardando modelo...')\n",
    "        self.trainer.save_model(output_dir)\n",
    "        print(f'Modelo guardado en {output_dir}')\n",
    "    \n",
    "    def get_predictions(self, encoded_texts):\n",
    "        predictions=[]\n",
    "        print('Getting predictions...')\n",
    "        for _,x in tqdm(enumerate(encoded_texts), total=len(encoded_texts)):\n",
    "            outputs = self.model(**x)\n",
    "            logits = outputs['logits']\n",
    "            y_pred = torch.argmax(logits, dim=-1)\n",
    "            predictions.append(y_pred[0].item()) \n",
    "        return predictions\n",
    "\n",
    "def tokenize_data(data, model):\n",
    "    dataset = Dataset.from_pandas(data, preserve_index=False)\n",
    "    return dataset.map(model.tokenize, batched=True, remove_columns='text')\n",
    "\n",
    "def tokenize_docs(texts, tokenizer):\n",
    "    return [tokenizer(str(text), truncation=True, return_tensors=\"pt\") for _, text in tqdm(enumerate(texts), total=len(texts))]\n",
    "\n",
    "def create_training_args(model, output_dir, epochs=num_train_epochs, batch_size=batch_size, \n",
    "                      learning_rate=2e-5, weight_decay=0.01, metric='accuracy', strategy = 'epoch', no_cuda=True):\n",
    "    model.training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,     \n",
    "        learning_rate=2e-5,                      #The initial learning rate for Adam\n",
    "        per_device_train_batch_size=batch_size,  #The batch size per GPU/TPU core/CPU for training.\n",
    "        per_device_eval_batch_size=batch_size,   #The batch size per GPU/TPU core/CPU for evaluation\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=metric,\n",
    "        weight_decay=weight_decay, #\n",
    "        evaluation_strategy=strategy,\n",
    "        save_strategy=strategy, \n",
    "        no_cuda = True\n",
    "    )\n",
    "    print('Training args added to the model')\n",
    "    \n",
    "def create_trainer(model, enc_train_dataset, enc_test_dataset):\n",
    "    model.trainer = Trainer(\n",
    "        model=model.model, \n",
    "        args=model.training_args, \n",
    "        compute_metrics=model.compute_metrics,\n",
    "        train_dataset=enc_train_dataset,\n",
    "        eval_dataset=enc_test_dataset,\n",
    "        tokenizer=model.tokenizer\n",
    "    )\n",
    "    print('Trainer added to the model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56886e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe6773f8e5a471e8fb5ccbfdc274431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing testing data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3147c26d6ac2489e85d50d121799834f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training args added to the model\n",
      "Trainer added to the model\n",
      "Empezando entrenamiento:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 5:15:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.004404</td>\n",
       "      <td>0.999109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.003687</td>\n",
       "      <td>0.999443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>0.999666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp/ipykernel_7440/3358408055.py:13: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\checkpoint-250\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\checkpoint-250\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\checkpoint-250\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\checkpoint-250\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\checkpoint-250\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\checkpoint-500\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\checkpoint-500\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\checkpoint-500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\checkpoint-750\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\checkpoint-750\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\checkpoint-750\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\checkpoint-750\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\checkpoint-750\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\checkpoint-750 (score: 0.9996659242761693).\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin del Entrenamiento\n",
      "Guardando modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "model = Model(model_checkpoint, NUM_LABELS)\n",
    "print('Tokenizing training data...')\n",
    "enc_train_dataset = tokenize_data(train_dataset, model)\n",
    "print('Tokenizing testing data...')\n",
    "enc_test_dataset = tokenize_data(test_dataset, model)\n",
    "create_training_args(model, output_dir)\n",
    "create_trainer(model, enc_train_dataset, enc_test_dataset)\n",
    "model.train()\n",
    "model.save(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27e7d321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\added_tokens.json. We won't load it.\n",
      "loading file C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\vocab.txt\n",
      "loading file C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\tokenizer.json\n",
      "loading file None\n",
      "loading file C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\special_tokens_map.json\n",
      "loading file C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\tokenizer_config.json\n",
      "loading configuration file C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"C:\\\\Users\\\\Usuario\\\\MASTER\\\\TFM\\\\models/distilBERT/EXP_2\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6df0bec0d6946428921365934fb9c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6543602dea3944d0886111ac3acd20c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.9996659242761693}\n",
      "f1: {'f1': 0.9996802046690118}\n",
      "precision: {'precision': 0.9997867803837953}\n",
      "recall: {'recall': 0.9995736516734172}\n",
      "Confusion matrix:\n",
      " [[4288    1]\n",
      " [   2 4689]]\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "metric_list = ['accuracy', 'f1', 'precision', 'recall']\n",
    "def get_metrics(eval_data, model, metric_list=metric_list):\n",
    "    y_real = eval_data.labels.values.tolist()\n",
    "    print('Tokenizing docs...')\n",
    "    enc_eval_data = tokenize_docs(eval_data.text.values, model.tokenizer)\n",
    "    y_pred = model.get_predictions(enc_eval_data)\n",
    "    results={}\n",
    "    print('Getting metrics:')\n",
    "    for metric in metric_list:\n",
    "        if metric=='accuracy':\n",
    "            m = load_metric(metric)\n",
    "        else:\n",
    "            m = load_metric(metric, 'macro')\n",
    "        results[metric] = m.compute(predictions=y_pred, references=y_real)\n",
    "        print(f'{metric}: {results[metric]}')\n",
    "    print(f'Confusion matrix:\\n {confusion_matrix(y_real, y_pred)}')\n",
    "    return results\n",
    "\n",
    "model = Model(output_dir, NUM_LABELS) ##########################\n",
    "m1 = get_metrics(test_dataset, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9231f",
   "metadata": {},
   "source": [
    "### BERT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d394b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de datos de entrenamiento: 2000. Numero de datos de test: 8980\n",
      "Train data label count:\n",
      " 0:968,    1:1032\n",
      "Test data label count:\n",
      " 0: 4289,     1:4691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033eab20d9ab43cab22eebd3e17cd17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8901773c5c419fa5eb10e77d11c907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training args added to the model\n",
      "Trainer added to the model\n",
      "Empezando entrenamiento:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 7:54:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.999443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.031900</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.999889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.031900</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.999889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\\checkpoint-250\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\\checkpoint-250\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\\checkpoint-250\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\\checkpoint-250\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\\checkpoint-250\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\\checkpoint-500\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\\checkpoint-500\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\\checkpoint-500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\\checkpoint-750\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\\checkpoint-750\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\\checkpoint-750\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\\checkpoint-750\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\\checkpoint-750\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\\checkpoint-500 (score: 0.9998886414253898).\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin del Entrenamiento\n",
      "Guardando modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_2\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4d2a5eab594cd1a29261161835ee92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8bd98f92214765a7083d6c4a1dca1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.9998886414253898}\n",
      "f1: {'f1': 0.9998934242779495}\n",
      "precision: {'precision': 0.9997868712702472}\n",
      "recall: {'recall': 1.0}\n",
      "Confusion matrix:\n",
      " [[4288    1]\n",
      " [   0 4691]]\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "model_checkpoint = 'bert-base-uncased'\n",
    "model_name = 'BERT'\n",
    "output_dir = os.path.join(os.getcwd(), f'models/{model_name}/EXP_{experiment}')\n",
    "\n",
    "#Train-test split\n",
    "#train_dataset = get_sample_data(train_data, n_training, label_dict = {0:0.5, 1:1}) \n",
    "train_dataset = get_sample_data(train_data, n_training)\n",
    "test_dataset = get_sample_data(test_data)\n",
    "print(\"Numero de datos de entrenamiento: {}. Numero de datos de test: {}\".format(len(train_dataset), len(test_dataset)))\n",
    "print(f'Train data label count:\\n 0:{train_dataset[train_dataset.labels==0].shape[0]},\\\n",
    "    1:{train_dataset[train_dataset.labels==1].shape[0]}')\n",
    "print(f'Test data label count:\\n 0: {test_dataset[test_dataset.labels==0].shape[0]}, \\\n",
    "    1:{test_dataset[test_dataset.labels==1].shape[0]}')\n",
    "\n",
    "#Training\n",
    "model = Model(model_checkpoint, NUM_LABELS)\n",
    "enc_train_dataset = tokenize_data(train_dataset, model)\n",
    "enc_test_dataset = tokenize_data(test_dataset, model)\n",
    "create_training_args(model, output_dir)\n",
    "create_trainer(model, enc_train_dataset, enc_test_dataset)\n",
    "model.train()\n",
    "model.save(output_dir)\n",
    "\n",
    "#Evaluation\n",
    "m2 = get_metrics(test_dataset, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d138608",
   "metadata": {},
   "source": [
    "### RoBERTa Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78206983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de datos de entrenamiento: 2000. Numero de datos de test: 8980\n",
      "Train data label count:\n",
      " 0:968,    1:1032\n",
      "Test data label count:\n",
      " 0: 4289,     1:4691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb27bdfb03046de9666d0f4c94b137f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7353c828d8654d59870acaf1ae9db85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training args added to the model\n",
      "Trainer added to the model\n",
      "Empezando entrenamiento:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 7:39:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.999666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.002872</td>\n",
       "      <td>0.999555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.003160</td>\n",
       "      <td>0.999555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\\checkpoint-250\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\\checkpoint-250\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\\checkpoint-250\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\\checkpoint-250\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\\checkpoint-250\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\\checkpoint-500\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\\checkpoint-500\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\\checkpoint-500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\\checkpoint-750\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\\checkpoint-750\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\\checkpoint-750\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\\checkpoint-750\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\\checkpoint-750\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\\checkpoint-250 (score: 0.9996659242761693).\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin del Entrenamiento\n",
      "Guardando modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_2\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68005b5683aa42a1a43e5ad52a039082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee90a5b486646b7b81cc6484e2f01ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.9996659242761693}\n",
      "f1: {'f1': 0.9996802728338485}\n",
      "precision: {'precision': 0.9995737425404945}\n",
      "recall: {'recall': 0.9997868258367086}\n",
      "Confusion matrix:\n",
      " [[4287    2]\n",
      " [   1 4690]]\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "model_checkpoint = 'roberta-base'\n",
    "model_name = 'RoBERTa'\n",
    "output_dir = os.path.join(os.getcwd(), f'models/{model_name}/EXP_{experiment}')\n",
    "\n",
    "#Train-test split\n",
    "#train_dataset = get_sample_data(train_data, n_training, label_dict = {0:0.5, 1:1}) \n",
    "train_dataset = get_sample_data(train_data, n_training) \n",
    "test_dataset = get_sample_data(test_data)\n",
    "print(\"Numero de datos de entrenamiento: {}. Numero de datos de test: {}\".format(len(train_dataset), len(test_dataset)))\n",
    "print(f'Train data label count:\\n 0:{train_dataset[train_dataset.labels==0].shape[0]},\\\n",
    "    1:{train_dataset[train_dataset.labels==1].shape[0]}')\n",
    "print(f'Test data label count:\\n 0: {test_dataset[test_dataset.labels==0].shape[0]}, \\\n",
    "    1:{test_dataset[test_dataset.labels==1].shape[0]}')\n",
    "\n",
    "#Training\n",
    "model = Model(model_checkpoint, NUM_LABELS)\n",
    "enc_train_dataset = tokenize_data(train_dataset, model)\n",
    "enc_test_dataset = tokenize_data(test_dataset, model)\n",
    "create_training_args(model, output_dir)\n",
    "create_trainer(model, enc_train_dataset, enc_test_dataset)\n",
    "model.train()\n",
    "model.save(output_dir)\n",
    "\n",
    "#Evaluation\n",
    "m3 = get_metrics(test_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7e92f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
