{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f89ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from random import randint as rnd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_metric\n",
    "from evaluate import load\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import TextClassificationPipeline\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "135d955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "NUM_LABELS=2\n",
    "keep_cols=['text','labels']\n",
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "model_name = 'distilBERT'\n",
    "training_data='ISOT'\n",
    "text_col='title'\n",
    "max_length = 512\n",
    "batch_size = 8 \n",
    "num_train_epochs=3 \n",
    "n_training = 1000\n",
    "seed=101\n",
    "experiment = 1\n",
    "output_dir = os.path.join(os.getcwd(), f'models/{model_name}/EXP_{experiment}/{n_training}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee74229",
   "metadata": {},
   "source": [
    "### LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d58986e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True examples: 21417\n",
      "Number of Fake examples: 23481\n",
      "Columns:['title', 'text', 'subject', 'date']\n",
      "Most repeated subjects:\n",
      "     True:  ['politicsNews', 'worldnews']\n",
      "     False: ['News', 'politics', 'left-news', 'Government News', 'US_News', 'Middle-east']\n",
      "Average number of words in titles:\n",
      "      True:  65,\n",
      "      Fake:  94\n",
      "Average number of words in texts: \n",
      "      True:  2383,\n",
      "      Fake:  2547\n",
      "\n",
      "Printing some examples:\n",
      "\n",
      "Label: False\n",
      "Title:  Donald Trump Sends Out Embarrassing New Year’s Eve Message; This is Disturbing\n",
      "Text: Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former rea...\n",
      "Subject: News\n",
      "date: December 31, 2017\n",
      "\n",
      "Label: True\n",
      "Title: As U.S. budget fight looms, Republicans flip their fiscal script\n",
      "Text: WASHINGTON (Reuters) - The head of a conservative Republican faction in the U.S. Congress, who voted this month for a huge expansion of the national debt to pay for tax cuts, called himself a “fiscal ...\n",
      "Subject: politicsNews\n",
      "date: December 31, 2017 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cargamos los datos\n",
    "true_path = os.path.join(os.getcwd(), f'data/{training_data}/True.csv')\n",
    "fake_path = os.path.join(os.getcwd(), f'data/{training_data}/Fake.csv')\n",
    "\n",
    "true_dataset = pd.read_csv(true_path)\n",
    "fake_dataset = pd.read_csv(fake_path)\n",
    "print(f'Number of True examples: {true_dataset.shape[0]}')\n",
    "print(f'Number of Fake examples: {fake_dataset.shape[0]}')\n",
    "print(f'Columns:{true_dataset.columns.to_list()}')\n",
    "print(f'Most repeated subjects:\\n \\\n",
    "    True:  {true_dataset.subject.value_counts().index.to_list()[:10]}\\n\\\n",
    "     False: {fake_dataset.subject.value_counts().index.to_list()[:10]}')\n",
    "print(f'Average number of words in titles:\\n\\\n",
    "      True:  {round(true_dataset.title.apply(lambda x: len(x)).mean())},\\n\\\n",
    "      Fake:  {round(fake_dataset.title.apply(lambda x: len(x)).mean())}')\n",
    "print(f'Average number of words in texts: \\n\\\n",
    "      True:  {round(true_dataset.text.apply(lambda x: len(x)).mean())},\\n\\\n",
    "      Fake:  {round(fake_dataset.text.apply(lambda x: len(x)).mean())}')\n",
    "\n",
    "def print_row(input_df: pd.DataFrame, index: int, label: int) -> None:\n",
    "    if label == 0:\n",
    "        print(f\"Label: True\")\n",
    "    else:\n",
    "        print(f\"Label: False\")\n",
    "    print(f\"Title: {input_df.iat[index, 0]}\")\n",
    "    print(f\"Text: {input_df.iat[index, 1][:200]}...\")\n",
    "    print(f\"Subject: {input_df.iat[index, 2]}\")\n",
    "    print(f\"date: {input_df.iat[index, 3]}\\n\")\n",
    "\n",
    "print('\\nPrinting some examples:\\n')\n",
    "print_row(input_df=fake_dataset, index=0, label=1)\n",
    "print_row(input_df=true_dataset, index=0, label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d767ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title column word count description:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    44898.000000\n",
       "mean        80.111720\n",
       "std         25.379685\n",
       "min          8.000000\n",
       "25%         63.000000\n",
       "50%         73.000000\n",
       "75%         91.000000\n",
       "max        286.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_data(fake, true, text_col=text_col, random_state=seed):\n",
    "    #Añadimos la columna label\n",
    "    true['labels'] = 0\n",
    "    fake['labels'] = 1\n",
    "    data = pd.concat([true, fake], axis=0).sample(frac=1, random_state=random_state).reset_index(drop=True) #shuffle\n",
    "    #Pasamos los textos a minúscula:\n",
    "    data[text_col] = data[text_col].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    data = data[[text_col,'labels']].rename(columns={text_col:'text'})\n",
    "    return data\n",
    "\n",
    "data = preprocess_data(fake_dataset, true_dataset)\n",
    "print(f'{text_col} column word count description:')\n",
    "data.text.apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5541acf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de datos de entrenamiento: 35918. Numero de datos de test: 8980\n",
      "Train data label count:\n",
      " 0:17128,    1:18790\n",
      "Test data label count:\n",
      " 0: 4289,     1:4691\n"
     ]
    }
   ],
   "source": [
    "#Separamos en entrenamiento y test\n",
    "\n",
    "def split_data(data, test_size=0.2, random_state=seed):\n",
    "    X = data['text']\n",
    "    y = data['labels']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = test_size, random_state = random_state)\n",
    "    train_dataset = pd.concat([X_train,y_train], axis=1).reset_index(drop=True)\n",
    "    test_dataset = pd.concat([X_test,y_test], axis=1).reset_index(drop=True)\n",
    "    print(\"Numero de datos de entrenamiento: {}. Numero de datos de test: {}\".format(len(train_dataset), len(test_dataset)))\n",
    "    print(f'Train data label count:\\n 0:{train_dataset[train_dataset.labels==0].shape[0]},\\\n",
    "    1:{train_dataset[train_dataset.labels==1].shape[0]}')\n",
    "    print(f'Test data label count:\\n 0: {test_dataset[test_dataset.labels==0].shape[0]}, \\\n",
    "    1:{test_dataset[test_dataset.labels==1].shape[0]}')\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_data, test_data = split_data(data, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed2df463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de datos de entrenamiento: 1000. Numero de datos de test: 8980\n",
      "Train data label count:\n",
      " 0:497,    1:503\n",
      "Test data label count:\n",
      " 0: 4289,     1:4691\n"
     ]
    }
   ],
   "source": [
    "def get_sample_data(data, n = data.shape[0], label_dict = {0:1, 1:1} ,random_state=seed):\n",
    "    df = pd.DataFrame(columns=data.columns)\n",
    "    for label,frac in label_dict.items():\n",
    "        tmp = data[data.labels == label].sample(frac=frac, random_state=random_state)\n",
    "        df = pd.concat([df,tmp])\n",
    "    return df.sample(frac=1, random_state=random_state)[:n].reset_index(drop=True)\n",
    "\n",
    "#train_dataset = get_sample_data(train_data, n_training, label_dict = {0:0.5, 1:1}) \n",
    "train_dataset = get_sample_data(train_data, n_training) \n",
    "test_dataset = get_sample_data(test_data)\n",
    "print(\"Numero de datos de entrenamiento: {}. Numero de datos de test: {}\".format(len(train_dataset), len(test_dataset)))\n",
    "print(f'Train data label count:\\n 0:{train_dataset[train_dataset.labels==0].shape[0]},\\\n",
    "    1:{train_dataset[train_dataset.labels==1].shape[0]}')\n",
    "print(f'Test data label count:\\n 0: {test_dataset[test_dataset.labels==0].shape[0]}, \\\n",
    "    1:{test_dataset[test_dataset.labels==1].shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b02dd8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, checkpoint, num_labels):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n",
    "        self.trainer = None\n",
    "        self.training_args = None\n",
    "    \n",
    "    def tokenize(self, data):\n",
    "        return self.tokenizer(data['text'], truncation=True)\n",
    "    \n",
    "    def compute_metrics(self, eval_pred):\n",
    "        metric = load_metric(\"accuracy\")\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    def train(self):\n",
    "        print('Empezando entrenamiento:')\n",
    "        self.trainer.train()\n",
    "        print('Fin del Entrenamiento')\n",
    "        \n",
    "    def save(self, output_dir):\n",
    "        print('Guardando modelo...')\n",
    "        self.trainer.save_model(output_dir)\n",
    "        print(f'Modelo guardado en {output_dir}')\n",
    "    \n",
    "    def get_predictions(self, encoded_texts):\n",
    "        predictions=[]\n",
    "        print('Getting predictions...')\n",
    "        for _,x in tqdm(enumerate(encoded_texts), total=len(encoded_texts)):\n",
    "            outputs = self.model(**x)\n",
    "            logits = outputs['logits']\n",
    "            y_pred = torch.argmax(logits, dim=-1)\n",
    "            predictions.append(y_pred[0].item()) \n",
    "        return predictions\n",
    "\n",
    "def tokenize_data(data, model):\n",
    "    dataset = Dataset.from_pandas(data, preserve_index=False)\n",
    "    return dataset.map(model.tokenize, batched=True, remove_columns='text')\n",
    "\n",
    "def tokenize_docs(texts, tokenizer):\n",
    "    return [tokenizer(str(text), truncation=True, return_tensors=\"pt\") for _, text in tqdm(enumerate(texts), total=len(texts))]\n",
    "\n",
    "def create_training_args(model, output_dir, epochs=num_train_epochs, batch_size=batch_size, \n",
    "                      learning_rate=2e-5, weight_decay=0.01, metric='accuracy', strategy = 'epoch', no_cuda=True):\n",
    "    model.training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,     \n",
    "        learning_rate=2e-5,                      #The initial learning rate for Adam\n",
    "        per_device_train_batch_size=batch_size,  #The batch size per GPU/TPU core/CPU for training.\n",
    "        per_device_eval_batch_size=batch_size,   #The batch size per GPU/TPU core/CPU for evaluation\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=metric,\n",
    "        weight_decay=weight_decay, #\n",
    "        evaluation_strategy=strategy,\n",
    "        save_strategy=strategy, \n",
    "        no_cuda = True\n",
    "    )\n",
    "    print('Training args added to the model')\n",
    "    \n",
    "def create_trainer(model, enc_train_dataset, enc_test_dataset):\n",
    "    model.trainer = Trainer(\n",
    "        model=model.model, \n",
    "        args=model.training_args, \n",
    "        compute_metrics=model.compute_metrics,\n",
    "        train_dataset=enc_train_dataset,\n",
    "        eval_dataset=enc_test_dataset,\n",
    "        tokenizer=model.tokenizer\n",
    "    )\n",
    "    print('Trainer added to the model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56886e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9547abcc91004cb89fbe4905db3bb7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing testing data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72d6fcaf32e40798dbe41fa63b30d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training args added to the model\n",
      "Trainer added to the model\n",
      "Empezando entrenamiento:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 17:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.139550</td>\n",
       "      <td>0.952561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.139145</td>\n",
       "      <td>0.958018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.173704</td>\n",
       "      <td>0.955679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp/ipykernel_14524/3358408055.py:13: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\\checkpoint-125\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\\checkpoint-125\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\\checkpoint-125\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\\checkpoint-125\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\\checkpoint-125\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\\checkpoint-250\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\\checkpoint-250\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\\checkpoint-250\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\\checkpoint-250\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\\checkpoint-250\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\\checkpoint-375\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\\checkpoint-375\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\\checkpoint-375\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\\checkpoint-375\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\\checkpoint-375\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\\checkpoint-250 (score: 0.9580178173719376).\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin del Entrenamiento\n",
      "Guardando modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/1000\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "model = Model(model_checkpoint, NUM_LABELS)\n",
    "print('Tokenizing training data...')\n",
    "enc_train_dataset = tokenize_data(train_dataset, model)\n",
    "print('Tokenizing testing data...')\n",
    "enc_test_dataset = tokenize_data(test_dataset, model)\n",
    "create_training_args(model, output_dir)\n",
    "create_trainer(model, enc_train_dataset, enc_test_dataset)\n",
    "model.train()\n",
    "model.save(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caf531f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae35814629c041f89b88512ce1db0091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8400e3317cf404abfaf4dc7ee7709eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.9580178173719376}\n",
      "f1: {'f1': 0.9600508636219137}\n",
      "precision: {'precision': 0.95448798988622}\n",
      "recall: {'recall': 0.9656789597100831}\n",
      "Confusion matrix:\n",
      " [[4073  216]\n",
      " [ 161 4530]]\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "metric_list = ['accuracy', 'f1', 'precision', 'recall']\n",
    "def get_metrics(eval_data, model, metric_list=metric_list):\n",
    "    y_real = eval_data.labels.values.tolist()\n",
    "    print('Tokenizing docs...')\n",
    "    enc_eval_data = tokenize_docs(eval_data.text.values, model.tokenizer)\n",
    "    y_pred = model.get_predictions(enc_eval_data)\n",
    "    results={}\n",
    "    print('Getting metrics:')\n",
    "    for metric in metric_list:\n",
    "        if metric=='accuracy':\n",
    "            m = load_metric(metric)\n",
    "        else:\n",
    "            m = load_metric(metric, 'macro')\n",
    "        results[metric] = m.compute(predictions=y_pred, references=y_real)\n",
    "        print(f'{metric}: {results[metric]}')\n",
    "    print(f'Confusion matrix:\\n {confusion_matrix(y_real, y_pred)}')\n",
    "    return results\n",
    "\n",
    "m11 = get_metrics(test_dataset, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bbf6a4",
   "metadata": {},
   "source": [
    "**n_training = 2000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eac098c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de datos de entrenamiento: 2000. Numero de datos de test: 8980\n",
      "Train data label count:\n",
      " 0:968,    1:1032\n",
      "Test data label count:\n",
      " 0: 4289,     1:4691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3595be7a0a74839aee366d953c75c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96b804aa8684947904ebdfb03c8d251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training args added to the model\n",
      "Trainer added to the model\n",
      "Empezando entrenamiento:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 24:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.108709</td>\n",
       "      <td>0.965924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.168500</td>\n",
       "      <td>0.176569</td>\n",
       "      <td>0.961247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.168500</td>\n",
       "      <td>0.128167</td>\n",
       "      <td>0.970490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\\checkpoint-250\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\\checkpoint-250\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\\checkpoint-250\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\\checkpoint-250\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\\checkpoint-250\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\\checkpoint-500\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\\checkpoint-500\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\\checkpoint-500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\\checkpoint-750\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\\checkpoint-750\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\\checkpoint-750\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\\checkpoint-750\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\\checkpoint-750\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\\checkpoint-750 (score: 0.970489977728285).\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin del Entrenamiento\n",
      "Guardando modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/2000\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f810ca16d0b1455ba5ecbf5a65cfea5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c73baf2a3b4654a5b630dc4031abd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.970489977728285}\n",
      "f1: {'f1': 0.9715268077790911}\n",
      "precision: {'precision': 0.979419410745234}\n",
      "recall: {'recall': 0.9637603922404605}\n",
      "Confusion matrix:\n",
      " [[4194   95]\n",
      " [ 170 4521]]\n"
     ]
    }
   ],
   "source": [
    "n_training = 2000\n",
    "output_dir = os.path.join(os.getcwd(), f'models/{model_name}/EXP_{experiment}/{n_training}')\n",
    "\n",
    "#Train-test split\n",
    "#train_dataset = get_sample_data(train_data, n_training, label_dict = {0:0.5, 1:1}) \n",
    "train_dataset = get_sample_data(train_data, n_training) \n",
    "test_dataset = get_sample_data(test_data)\n",
    "print(\"Numero de datos de entrenamiento: {}. Numero de datos de test: {}\".format(len(train_dataset), len(test_dataset)))\n",
    "print(f'Train data label count:\\n 0:{train_dataset[train_dataset.labels==0].shape[0]},\\\n",
    "    1:{train_dataset[train_dataset.labels==1].shape[0]}')\n",
    "print(f'Test data label count:\\n 0: {test_dataset[test_dataset.labels==0].shape[0]}, \\\n",
    "    1:{test_dataset[test_dataset.labels==1].shape[0]}')\n",
    "\n",
    "#Training\n",
    "model = Model(model_checkpoint, NUM_LABELS)\n",
    "enc_train_dataset = tokenize_data(train_dataset, model)\n",
    "enc_test_dataset = tokenize_data(test_dataset, model)\n",
    "create_training_args(model, output_dir)\n",
    "create_trainer(model, enc_train_dataset, enc_test_dataset)\n",
    "model.train()\n",
    "model.save(output_dir)\n",
    "\n",
    "#Evaluation\n",
    "m12 = get_metrics(test_dataset, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd182629",
   "metadata": {},
   "source": [
    "**n_training = 5000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1d44b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de datos de entrenamiento: 5000. Numero de datos de test: 8980\n",
      "Train data label count:\n",
      " 0:2400,    1:2600\n",
      "Test data label count:\n",
      " 0: 4289,     1:4691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7306730319d9456090761f769d2c443e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6856baaffa5450abb9d17d27bdef615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 5000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training args added to the model\n",
      "Trainer added to the model\n",
      "Empezando entrenamiento:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 43:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.179600</td>\n",
       "      <td>0.089398</td>\n",
       "      <td>0.976392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>0.119214</td>\n",
       "      <td>0.975947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.122673</td>\n",
       "      <td>0.977728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\\checkpoint-625\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\\checkpoint-625\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\\checkpoint-625\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\\checkpoint-625\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\\checkpoint-625\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\\checkpoint-1250\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\\checkpoint-1250\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\\checkpoint-1250\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\\checkpoint-1250\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\\checkpoint-1250\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\\checkpoint-1875\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\\checkpoint-1875\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\\checkpoint-1875\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\\checkpoint-1875\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\\checkpoint-1875\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\\checkpoint-1875 (score: 0.977728285077951).\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin del Entrenamiento\n",
      "Guardando modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en C:\\Users\\Usuario\\MASTER\\TFM\\models/distilBERT/EXP_1/5000\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e5df15e2104768a10b7bfb5485655c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c53443278843f9914dfbc4a5613eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.977728285077951}\n",
      "f1: {'f1': 0.9783970620004321}\n",
      "precision: {'precision': 0.9916794394569739}\n",
      "recall: {'recall': 0.9654657855467917}\n",
      "Confusion matrix:\n",
      " [[4251   38]\n",
      " [ 162 4529]]\n"
     ]
    }
   ],
   "source": [
    "n_training = 5000\n",
    "output_dir = os.path.join(os.getcwd(), f'models/{model_name}/EXP_{experiment}/{n_training}')\n",
    "\n",
    "#Train-test split\n",
    "#train_dataset = get_sample_data(train_data, n_training, label_dict = {0:0.5, 1:1}) \n",
    "train_dataset = get_sample_data(train_data, n_training) \n",
    "test_dataset = get_sample_data(test_data)\n",
    "print(\"Numero de datos de entrenamiento: {}. Numero de datos de test: {}\".format(len(train_dataset), len(test_dataset)))\n",
    "print(f'Train data label count:\\n 0:{train_dataset[train_dataset.labels==0].shape[0]},\\\n",
    "    1:{train_dataset[train_dataset.labels==1].shape[0]}')\n",
    "print(f'Test data label count:\\n 0: {test_dataset[test_dataset.labels==0].shape[0]}, \\\n",
    "    1:{test_dataset[test_dataset.labels==1].shape[0]}')\n",
    "\n",
    "#Training\n",
    "model = Model(model_checkpoint, NUM_LABELS)\n",
    "enc_train_dataset = tokenize_data(train_dataset, model)\n",
    "enc_test_dataset = tokenize_data(test_dataset, model)\n",
    "create_training_args(model, output_dir)\n",
    "create_trainer(model, enc_train_dataset, enc_test_dataset)\n",
    "model.train()\n",
    "model.save(output_dir)\n",
    "\n",
    "#Evaluation\n",
    "m13 = get_metrics(test_dataset, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9231f",
   "metadata": {},
   "source": [
    "### BERT Model\n",
    "\n",
    "**n_training=1000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "136a1ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de datos de entrenamiento: 1000. Numero de datos de test: 8980\n",
      "Train data label count:\n",
      " 0:497,    1:503\n",
      "Test data label count:\n",
      " 0: 4289,     1:4691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f939ce24a34d69a0607dd5c782b7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae766aec7ca648a3853ddaa53302a244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training args added to the model\n",
      "Trainer added to the model\n",
      "Empezando entrenamiento:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 30:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.281490</td>\n",
       "      <td>0.926949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.170406</td>\n",
       "      <td>0.956570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.176841</td>\n",
       "      <td>0.964143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\\checkpoint-125\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\\checkpoint-125\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\\checkpoint-125\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\\checkpoint-125\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\\checkpoint-125\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\\checkpoint-250\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\\checkpoint-250\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\\checkpoint-250\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\\checkpoint-250\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\\checkpoint-250\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\\checkpoint-375\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\\checkpoint-375\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\\checkpoint-375\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\\checkpoint-375\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\\checkpoint-375\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\\checkpoint-375 (score: 0.9641425389755011).\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin del Entrenamiento\n",
      "Guardando modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/1000\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6975527b3b87497aa3afe1df1846f19a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21bcb91be1564ad6892b1dbe956eac5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.9641425389755011}\n",
      "f1: {'f1': 0.9650380021715526}\n",
      "precision: {'precision': 0.9834034078335915}\n",
      "recall: {'recall': 0.947345981667022}\n",
      "Confusion matrix:\n",
      " [[4214   75]\n",
      " [ 247 4444]]\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "model_checkpoint = 'bert-base-uncased'\n",
    "model_name = 'BERT'\n",
    "n_training = 1000\n",
    "output_dir = os.path.join(os.getcwd(), f'models/{model_name}/EXP_{experiment}/{n_training}')\n",
    "\n",
    "#Train-test split\n",
    "#train_dataset = get_sample_data(train_data, n_training, label_dict = {0:0.5, 1:1}) \n",
    "train_dataset = get_sample_data(train_data, n_training) \n",
    "test_dataset = get_sample_data(test_data)\n",
    "print(\"Numero de datos de entrenamiento: {}. Numero de datos de test: {}\".format(len(train_dataset), len(test_dataset)))\n",
    "print(f'Train data label count:\\n 0:{train_dataset[train_dataset.labels==0].shape[0]},\\\n",
    "    1:{train_dataset[train_dataset.labels==1].shape[0]}')\n",
    "print(f'Test data label count:\\n 0: {test_dataset[test_dataset.labels==0].shape[0]}, \\\n",
    "    1:{test_dataset[test_dataset.labels==1].shape[0]}')\n",
    "\n",
    "#Training\n",
    "model = Model(model_checkpoint, NUM_LABELS)\n",
    "enc_train_dataset = tokenize_data(train_dataset, model)\n",
    "enc_test_dataset = tokenize_data(test_dataset, model)\n",
    "create_training_args(model, output_dir)\n",
    "create_trainer(model, enc_train_dataset, enc_test_dataset)\n",
    "model.train()\n",
    "model.save(output_dir)\n",
    "\n",
    "#Evaluation\n",
    "m21 = get_metrics(test_dataset, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8e8815",
   "metadata": {},
   "source": [
    "**n_training=2000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68e30f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de datos de entrenamiento: 2000. Numero de datos de test: 8980\n",
      "Train data label count:\n",
      " 0:968,    1:1032\n",
      "Test data label count:\n",
      " 0: 4289,     1:4691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1f6b26a0f141968f37eb77208adee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0f1df755b14691ac85e3759a89caa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training args added to the model\n",
      "Trainer added to the model\n",
      "Empezando entrenamiento:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 44:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.123307</td>\n",
       "      <td>0.967261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.168100</td>\n",
       "      <td>0.212155</td>\n",
       "      <td>0.958352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.168100</td>\n",
       "      <td>0.144527</td>\n",
       "      <td>0.970379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\\checkpoint-250\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\\checkpoint-250\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\\checkpoint-250\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\\checkpoint-250\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\\checkpoint-250\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\\checkpoint-500\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\\checkpoint-500\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\\checkpoint-500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\\checkpoint-750\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\\checkpoint-750\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\\checkpoint-750\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\\checkpoint-750\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\\checkpoint-750\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\\checkpoint-750 (score: 0.9703786191536748).\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin del Entrenamiento\n",
      "Guardando modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/2000\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04cbc42072448c781f3c0706e5071e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b4eb64c8f7411d8ea571b172275321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.9703786191536748}\n",
      "f1: {'f1': 0.9711809317443121}\n",
      "precision: {'precision': 0.9874421678783873}\n",
      "recall: {'recall': 0.9554465998720955}\n",
      "Confusion matrix:\n",
      " [[4232   57]\n",
      " [ 209 4482]]\n"
     ]
    }
   ],
   "source": [
    "n_training = 2000\n",
    "output_dir = os.path.join(os.getcwd(), f'models/{model_name}/EXP_{experiment}/{n_training}')\n",
    "\n",
    "#Train-test split\n",
    "#train_dataset = get_sample_data(train_data, n_training, label_dict = {0:0.5, 1:1}) \n",
    "train_dataset = get_sample_data(train_data, n_training) \n",
    "test_dataset = get_sample_data(test_data)\n",
    "print(\"Numero de datos de entrenamiento: {}. Numero de datos de test: {}\".format(len(train_dataset), len(test_dataset)))\n",
    "print(f'Train data label count:\\n 0:{train_dataset[train_dataset.labels==0].shape[0]},\\\n",
    "    1:{train_dataset[train_dataset.labels==1].shape[0]}')\n",
    "print(f'Test data label count:\\n 0: {test_dataset[test_dataset.labels==0].shape[0]}, \\\n",
    "    1:{test_dataset[test_dataset.labels==1].shape[0]}')\n",
    "\n",
    "#Training\n",
    "model = Model(model_checkpoint, NUM_LABELS)\n",
    "enc_train_dataset = tokenize_data(train_dataset, model)\n",
    "enc_test_dataset = tokenize_data(test_dataset, model)\n",
    "create_training_args(model, output_dir)\n",
    "create_trainer(model, enc_train_dataset, enc_test_dataset)\n",
    "model.train()\n",
    "model.save(output_dir)\n",
    "\n",
    "#Evaluation\n",
    "m22 = get_metrics(test_dataset, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32a0d0f",
   "metadata": {},
   "source": [
    "**n_training=5000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8f13cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de datos de entrenamiento: 5000. Numero de datos de test: 8980\n",
      "Train data label count:\n",
      " 0:2400,    1:2600\n",
      "Test data label count:\n",
      " 0: 4289,     1:4691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6b08313608401fa7413670266f75fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559acd20834d4e0db9025ae72a85dbc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 5000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training args added to the model\n",
      "Trainer added to the model\n",
      "Empezando entrenamiento:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 1:22:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.189600</td>\n",
       "      <td>0.109704</td>\n",
       "      <td>0.976503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.069600</td>\n",
       "      <td>0.128432</td>\n",
       "      <td>0.978731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.147003</td>\n",
       "      <td>0.976949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\\checkpoint-625\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\\checkpoint-625\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\\checkpoint-625\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\\checkpoint-625\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\\checkpoint-625\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\\checkpoint-1250\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\\checkpoint-1250\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\\checkpoint-1250\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\\checkpoint-1250\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\\checkpoint-1250\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\\checkpoint-1875\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\\checkpoint-1875\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\\checkpoint-1875\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\\checkpoint-1875\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\\checkpoint-1875\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\\checkpoint-1250 (score: 0.9787305122494432).\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin del Entrenamiento\n",
      "Guardando modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en C:\\Users\\Usuario\\MASTER\\TFM\\models/BERT/EXP_1/5000\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97ae73f3ff540b482ace7d49096ae2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d08d86aa294f968a65a36e42f3012d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.9787305122494432}\n",
      "f1: {'f1': 0.9794025665911787}\n",
      "precision: {'precision': 0.9910519423832388}\n",
      "recall: {'recall': 0.9680238755062887}\n",
      "Confusion matrix:\n",
      " [[4248   41]\n",
      " [ 150 4541]]\n"
     ]
    }
   ],
   "source": [
    "n_training = 5000\n",
    "output_dir = os.path.join(os.getcwd(), f'models/{model_name}/EXP_{experiment}/{n_training}')\n",
    "\n",
    "#Train-test split\n",
    "#train_dataset = get_sample_data(train_data, n_training, label_dict = {0:0.5, 1:1}) \n",
    "train_dataset = get_sample_data(train_data, n_training) \n",
    "test_dataset = get_sample_data(test_data)\n",
    "print(\"Numero de datos de entrenamiento: {}. Numero de datos de test: {}\".format(len(train_dataset), len(test_dataset)))\n",
    "print(f'Train data label count:\\n 0:{train_dataset[train_dataset.labels==0].shape[0]},\\\n",
    "    1:{train_dataset[train_dataset.labels==1].shape[0]}')\n",
    "print(f'Test data label count:\\n 0: {test_dataset[test_dataset.labels==0].shape[0]}, \\\n",
    "    1:{test_dataset[test_dataset.labels==1].shape[0]}')\n",
    "\n",
    "#Training\n",
    "model = Model(model_checkpoint, NUM_LABELS)\n",
    "enc_train_dataset = tokenize_data(train_dataset, model)\n",
    "enc_test_dataset = tokenize_data(test_dataset, model)\n",
    "create_training_args(model, output_dir)\n",
    "create_trainer(model, enc_train_dataset, enc_test_dataset)\n",
    "model.train()\n",
    "model.save(output_dir)\n",
    "\n",
    "#Evaluation\n",
    "m23 = get_metrics(test_dataset, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d138608",
   "metadata": {},
   "source": [
    "### RoBERTa Model\n",
    "\n",
    "**n_training=1000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bffea60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de datos de entrenamiento: 1000. Numero de datos de test: 8980\n",
      "Train data label count:\n",
      " 0:497,    1:503\n",
      "Test data label count:\n",
      " 0: 4289,     1:4691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81944ea6fde470092aeffb700308b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ce5926946c4b0eb162fa672bb51aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training args added to the model\n",
      "Trainer added to the model\n",
      "Empezando entrenamiento:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 45:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.146580</td>\n",
       "      <td>0.965702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.188760</td>\n",
       "      <td>0.963363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.162409</td>\n",
       "      <td>0.972272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\\checkpoint-125\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\\checkpoint-125\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\\checkpoint-125\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\\checkpoint-125\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\\checkpoint-125\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\\checkpoint-250\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\\checkpoint-250\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\\checkpoint-250\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\\checkpoint-250\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\\checkpoint-250\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\\checkpoint-375\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\\checkpoint-375\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\\checkpoint-375\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\\checkpoint-375\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\\checkpoint-375\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\\checkpoint-375 (score: 0.972271714922049).\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin del Entrenamiento\n",
      "Guardando modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/1000\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c871bcc0d64cd8a7ba145837d014a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72e848f6dd04993ba5e83c6e842e4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.972271714922049}\n",
      "f1: {'f1': 0.973228685087625}\n",
      "precision: {'precision': 0.9817787418655097}\n",
      "recall: {'recall': 0.9648262630569175}\n",
      "Confusion matrix:\n",
      " [[4205   84]\n",
      " [ 165 4526]]\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "model_checkpoint = 'roberta-base'\n",
    "model_name = 'RoBERTa'\n",
    "n_training = 1000\n",
    "output_dir = os.path.join(os.getcwd(), f'models/{model_name}/EXP_{experiment}/{n_training}')\n",
    "\n",
    "#Train-test split\n",
    "#train_dataset = get_sample_data(train_data, n_training, label_dict = {0:0.5, 1:1}) \n",
    "train_dataset = get_sample_data(train_data, n_training) \n",
    "test_dataset = get_sample_data(test_data)\n",
    "print(\"Numero de datos de entrenamiento: {}. Numero de datos de test: {}\".format(len(train_dataset), len(test_dataset)))\n",
    "print(f'Train data label count:\\n 0:{train_dataset[train_dataset.labels==0].shape[0]},\\\n",
    "    1:{train_dataset[train_dataset.labels==1].shape[0]}')\n",
    "print(f'Test data label count:\\n 0: {test_dataset[test_dataset.labels==0].shape[0]}, \\\n",
    "    1:{test_dataset[test_dataset.labels==1].shape[0]}')\n",
    "\n",
    "#Training\n",
    "model = Model(model_checkpoint, NUM_LABELS)\n",
    "enc_train_dataset = tokenize_data(train_dataset, model)\n",
    "enc_test_dataset = tokenize_data(test_dataset, model)\n",
    "create_training_args(model, output_dir)\n",
    "create_trainer(model, enc_train_dataset, enc_test_dataset)\n",
    "model.train()\n",
    "model.save(output_dir)\n",
    "\n",
    "#Evaluation\n",
    "m31 = get_metrics(test_dataset, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8c5315",
   "metadata": {},
   "source": [
    "**n_training=2000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e2e33a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de datos de entrenamiento: 2000. Numero de datos de test: 8980\n",
      "Train data label count:\n",
      " 0:968,    1:1032\n",
      "Test data label count:\n",
      " 0: 4289,     1:4691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa82de113448405681711d9ec711d4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d6b33d3a134a628ab9f41614c8a17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training args added to the model\n",
      "Trainer added to the model\n",
      "Empezando entrenamiento:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 46:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.082833</td>\n",
       "      <td>0.979176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.149300</td>\n",
       "      <td>0.109491</td>\n",
       "      <td>0.978731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.149300</td>\n",
       "      <td>0.089002</td>\n",
       "      <td>0.983185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\\checkpoint-250\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\\checkpoint-250\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\\checkpoint-250\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\\checkpoint-250\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\\checkpoint-250\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\\checkpoint-500\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\\checkpoint-500\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\\checkpoint-500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\\checkpoint-750\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\\checkpoint-750\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\\checkpoint-750\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\\checkpoint-750\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\\checkpoint-750\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\\checkpoint-750 (score: 0.983184855233853).\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin del Entrenamiento\n",
      "Guardando modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/2000\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f09bff47de4924a868f838ffaa964f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70e396d625a43d3a15b69bfb8503aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.983184855233853}\n",
      "f1: {'f1': 0.9838069705093834}\n",
      "precision: {'precision': 0.9898575744497194}\n",
      "recall: {'recall': 0.9778298870176935}\n",
      "Confusion matrix:\n",
      " [[4242   47]\n",
      " [ 104 4587]]\n"
     ]
    }
   ],
   "source": [
    "n_training = 2000\n",
    "output_dir = os.path.join(os.getcwd(), f'models/{model_name}/EXP_{experiment}/{n_training}')\n",
    "\n",
    "#Train-test split\n",
    "#train_dataset = get_sample_data(train_data, n_training, label_dict = {0:0.5, 1:1}) \n",
    "train_dataset = get_sample_data(train_data, n_training) \n",
    "test_dataset = get_sample_data(test_data)\n",
    "print(\"Numero de datos de entrenamiento: {}. Numero de datos de test: {}\".format(len(train_dataset), len(test_dataset)))\n",
    "print(f'Train data label count:\\n 0:{train_dataset[train_dataset.labels==0].shape[0]},\\\n",
    "    1:{train_dataset[train_dataset.labels==1].shape[0]}')\n",
    "print(f'Test data label count:\\n 0: {test_dataset[test_dataset.labels==0].shape[0]}, \\\n",
    "    1:{test_dataset[test_dataset.labels==1].shape[0]}')\n",
    "\n",
    "#Training\n",
    "model = Model(model_checkpoint, NUM_LABELS)\n",
    "enc_train_dataset = tokenize_data(train_dataset, model)\n",
    "enc_test_dataset = tokenize_data(test_dataset, model)\n",
    "create_training_args(model, output_dir)\n",
    "create_trainer(model, enc_train_dataset, enc_test_dataset)\n",
    "model.train()\n",
    "model.save(output_dir)\n",
    "\n",
    "#Evaluation\n",
    "m32 = get_metrics(test_dataset, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c5028b",
   "metadata": {},
   "source": [
    "**n_training=5000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51ea3616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de datos de entrenamiento: 5000. Numero de datos de test: 8980\n",
      "Train data label count:\n",
      " 0:2400,    1:2600\n",
      "Test data label count:\n",
      " 0: 4289,     1:4691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\Usuario/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d433630252d468290bbde1daf12cc6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56f70e54ab941ce887a111778e68774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 5000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training args added to the model\n",
      "Trainer added to the model\n",
      "Empezando entrenamiento:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 1:30:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.095363</td>\n",
       "      <td>0.983073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>0.065970</td>\n",
       "      <td>0.986860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.067767</td>\n",
       "      <td>0.988419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\\checkpoint-625\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\\checkpoint-625\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\\checkpoint-625\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\\checkpoint-625\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\\checkpoint-625\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\\checkpoint-1250\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\\checkpoint-1250\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\\checkpoint-1250\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\\checkpoint-1250\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\\checkpoint-1250\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8980\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\\checkpoint-1875\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\\checkpoint-1875\\config.json\n",
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\\checkpoint-1875\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\\checkpoint-1875\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\\checkpoint-1875\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\\checkpoint-1875 (score: 0.9884187082405346).\n",
      "Saving model checkpoint to C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\n",
      "Configuration saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin del Entrenamiento\n",
      "Guardando modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en C:\\Users\\Usuario\\MASTER\\TFM\\models/RoBERTa/EXP_1/5000\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921baf5ada3c49f09d09bd5adfee2c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600d78d7f601472c99a2631158444e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.9884187082405346}\n",
      "f1: {'f1': 0.9888650963597431}\n",
      "precision: {'precision': 0.9933318993331899}\n",
      "recall: {'recall': 0.9844382860797272}\n",
      "Confusion matrix:\n",
      " [[4258   31]\n",
      " [  73 4618]]\n"
     ]
    }
   ],
   "source": [
    "n_training = 5000\n",
    "output_dir = os.path.join(os.getcwd(), f'models/{model_name}/EXP_{experiment}/{n_training}')\n",
    "\n",
    "#Train-test split\n",
    "#train_dataset = get_sample_data(train_data, n_training, label_dict = {0:0.5, 1:1}) \n",
    "train_dataset = get_sample_data(train_data, n_training) \n",
    "test_dataset = get_sample_data(test_data)\n",
    "print(\"Numero de datos de entrenamiento: {}. Numero de datos de test: {}\".format(len(train_dataset), len(test_dataset)))\n",
    "print(f'Train data label count:\\n 0:{train_dataset[train_dataset.labels==0].shape[0]},\\\n",
    "    1:{train_dataset[train_dataset.labels==1].shape[0]}')\n",
    "print(f'Test data label count:\\n 0: {test_dataset[test_dataset.labels==0].shape[0]}, \\\n",
    "    1:{test_dataset[test_dataset.labels==1].shape[0]}')\n",
    "\n",
    "#Training\n",
    "model = Model(model_checkpoint, NUM_LABELS)\n",
    "enc_train_dataset = tokenize_data(train_dataset, model)\n",
    "enc_test_dataset = tokenize_data(test_dataset, model)\n",
    "create_training_args(model, output_dir)\n",
    "create_trainer(model, enc_train_dataset, enc_test_dataset)\n",
    "model.train()\n",
    "model.save(output_dir)\n",
    "\n",
    "#Evaluation\n",
    "m33 = get_metrics(test_dataset, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
