{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f89ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "135d955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "NUM_LABELS=2\n",
    "keep_cols=['text','labels']\n",
    "metric_list = ['accuracy', 'f1', 'precision', 'recall']\n",
    "model_name = 'distilBERT'\n",
    "training_data='ISOT'\n",
    "eval_data_1 ='FakeNewsNet'\n",
    "eval_data_2 ='LIAR'\n",
    "text_col='title'\n",
    "max_length = 512\n",
    "batch_size = 8 \n",
    "num_train_epochs=3 \n",
    "seed=101\n",
    "experiment = 3\n",
    "input_dir_1_2 = os.path.join(os.getcwd(), f'models/{model_name}/EXP_1/2000') \n",
    "input_dir_1_5 = os.path.join(os.getcwd(), f'models/{model_name}/EXP_1/5000') \n",
    "input_dir_2 = os.path.join(os.getcwd(), f'models/{model_name}/EXP_2') \n",
    "#output_dir = os.path.join(os.getcwd(), f'models/{model_name}/EXP_{experiment}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62bd80bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True examples in PolitiFact: 624\n",
      "Number of True examples in GossipCop:  16817\n",
      "Number of Fake examples in PolitiFact: 432\n",
      "Number of Fake examples in GossipCop:  5323\n",
      "PoliticFact dataset:\n",
      "Average number of words in titles:\n",
      "      True:  51\n",
      "      Fake:  73\n",
      "GossipCop dataset:\n",
      "Average number of words in titles:\n",
      "      True:  69\n",
      "      Fake:  69\n"
     ]
    }
   ],
   "source": [
    "#FakeNewsNet\n",
    "gossip_fake = pd.read_csv(os.path.join(os.getcwd(), f'data/{eval_data_1}/gossipcop_fake.csv'))\n",
    "gossip_true = pd.read_csv(os.path.join(os.getcwd(), f'data/{eval_data_1}/gossipcop_real.csv'))\n",
    "politic_fake = pd.read_csv(os.path.join(os.getcwd(),f'data/{eval_data_1}/politifact_fake.csv'))\n",
    "politic_true = pd.read_csv(os.path.join(os.getcwd(),f'data/{eval_data_1}/politifact_real.csv'))\n",
    "\n",
    "print(f'Number of True examples in PolitiFact: {politic_true.shape[0]}')\n",
    "print(f'Number of True examples in GossipCop:  {gossip_true.shape[0]}')\n",
    "print(f'Number of Fake examples in PolitiFact: {politic_fake.shape[0]}')\n",
    "print(f'Number of Fake examples in GossipCop:  {gossip_fake.shape[0]}')\n",
    "\n",
    "print('PoliticFact dataset:')\n",
    "print(f'Average number of words in titles:\\n\\\n",
    "      True:  {round(politic_true.title.apply(lambda x: len(x)).mean())}\\n\\\n",
    "      Fake:  {round(politic_fake.title.apply(lambda x: len(x)).mean())}')\n",
    "print('GossipCop dataset:')\n",
    "print(f'Average number of words in titles:\\n\\\n",
    "      True:  {round(gossip_true.title.apply(lambda x: len(x)).mean())}\\n\\\n",
    "      Fake:  {round(gossip_fake.title.apply(lambda x: len(x)).mean())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef81ad7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True examples in Liar: 2053\n",
      "Number of Fake examples in Liar: 2507\n",
      "Average number of words in statement:\n",
      "      True:  106\n",
      "      Fake:  101\n",
      "Most repeated subjects:\n",
      "      True:  ['elections', 'health-care', 'taxes', 'education', 'candidates-biography', 'economy', 'guns', 'economy,jobs', 'federal-budget', 'immigration', 'abortion', 'iraq', 'energy', 'crime', 'jobs']\n",
      "     False: ['health-care', 'immigration', 'elections', 'taxes', 'education', 'candidates-biography', 'state-budget', 'economy', 'abortion', 'guns', 'jobs', 'federal-budget', 'foreign-policy', 'energy', 'economy,jobs']\n"
     ]
    }
   ],
   "source": [
    "#LIAR\n",
    "column_names = ['ID', 'labels', 'title', 'subject', 'speaker', 'speaker_job', 'state', 'political_party', \n",
    "                'barely true counts', 'false_counts', 'half true counts', 'mostly true counts', 'pants on fire counts',\n",
    "                 'context']\n",
    "liar_test = pd.read_table(os.path.join(os.getcwd(), f'data/{eval_data_2}/test.tsv'), header=None, names=column_names)\n",
    "liar_train = pd.read_table(os.path.join(os.getcwd(), f'data/{eval_data_2}/train.tsv'), header=None, names=column_names)\n",
    "liar_valid = pd.read_table(os.path.join(os.getcwd(),f'data/{eval_data_2}/valid.tsv'), header=None, names=column_names)\n",
    "liar = pd.concat([liar_test, liar_train, liar_valid], axis=0).reset_index(drop=True)\n",
    "liar_fake = liar[liar.labels=='false']\n",
    "liar_true = liar[liar.labels=='true']\n",
    "\n",
    "print(f'Number of True examples in Liar: {liar_true.shape[0]}')\n",
    "print(f'Number of Fake examples in Liar: {liar_fake.shape[0]}')\n",
    "\n",
    "print(f'Average number of words in statement:\\n\\\n",
    "      True:  {round(liar_true.title.apply(lambda x: len(x)).mean())}\\n\\\n",
    "      Fake:  {round(liar_fake.title.apply(lambda x: len(x)).mean())}')\n",
    "\n",
    "print(f'Most repeated subjects:\\n \\\n",
    "     True:  {liar_true.subject.value_counts().index.to_list()[:15]}\\n\\\n",
    "     False: {liar_fake.subject.value_counts().index.to_list()[:15]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d767ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolitiFact Data data label count:\n",
      "0: 624\n",
      "1: 432\n",
      "GossipCop Data data label count:\n",
      "0: 16817\n",
      "1: 5323\n",
      "LIAR Data data label count:\n",
      "0: 2053\n",
      "1: 2507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp/ipykernel_4456/3489758553.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  true['labels'] = 0\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp/ipykernel_4456/3489758553.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fake['labels'] = 1\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(fake, true, text_col=text_col, random_state=seed):\n",
    "    #A√±adimos la columna label\n",
    "    true['labels'] = 0\n",
    "    fake['labels'] = 1\n",
    "    data = pd.concat([true, fake], axis=0).sample(frac=1, random_state=random_state).reset_index(drop=True) #shuffle\n",
    "    #Pasamos los textos a min√∫scula:\n",
    "    data[text_col] = data[text_col].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    data = data[[text_col,'labels']].rename(columns={text_col:'text'})\n",
    "    return data\n",
    "\n",
    "gossip_data = preprocess_data(gossip_fake, gossip_true)\n",
    "politic_data = preprocess_data(politic_fake, politic_true)\n",
    "liar_data = preprocess_data(liar_fake, liar_true)\n",
    "\n",
    "print(f'PolitiFact Data data label count:\\n\\\n",
    "0: {politic_data[politic_data.labels==0].shape[0]}\\n\\\n",
    "1: {politic_data[politic_data.labels==1].shape[0]}')\n",
    "\n",
    "print(f'GossipCop Data data label count:\\n\\\n",
    "0: {gossip_data[gossip_data.labels==0].shape[0]}\\n\\\n",
    "1: {gossip_data[gossip_data.labels==1].shape[0]}')\n",
    "\n",
    "print(f'LIAR Data data label count:\\n\\\n",
    "0: {liar_data[liar_data.labels==0].shape[0]}\\n\\\n",
    "1: {liar_data[liar_data.labels==1].shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b02dd8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, checkpoint, num_labels):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n",
    "        self.trainer = None\n",
    "        self.training_args = None\n",
    "    \n",
    "    def tokenize(self, data):\n",
    "        return self.tokenizer(data['text'], truncation=True)\n",
    "    \n",
    "    def compute_metrics(self, eval_pred):\n",
    "        metric = load_metric(\"accuracy\")\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    def train(self):\n",
    "        print('Empezando entrenamiento:')\n",
    "        self.trainer.train()\n",
    "        print('Fin del Entrenamiento')\n",
    "        \n",
    "    def save(self, output_dir):\n",
    "        print('Guardando modelo...')\n",
    "        self.trainer.save_model(output_dir)\n",
    "        print(f'Modelo guardado en {output_dir}')\n",
    "    \n",
    "    def get_predictions(self, encoded_texts):\n",
    "        predictions=[]\n",
    "        print('Getting predictions...')\n",
    "        for _,x in tqdm(enumerate(encoded_texts), total=len(encoded_texts)):\n",
    "            outputs = self.model(**x)\n",
    "            logits = outputs['logits']\n",
    "            y_pred = torch.argmax(logits, dim=-1)\n",
    "            predictions.append(y_pred[0].item()) \n",
    "        return predictions\n",
    "\n",
    "def tokenize_data(data, model):\n",
    "    dataset = Dataset.from_pandas(data, preserve_index=False)\n",
    "    return dataset.map(model.tokenize, batched=True, remove_columns='text')\n",
    "\n",
    "def tokenize_docs(texts, tokenizer):\n",
    "    return [tokenizer(str(text), truncation=True, return_tensors=\"pt\") for _, text in tqdm(enumerate(texts), total=len(texts))]\n",
    "\n",
    "#Evaluation\n",
    "def get_metrics(eval_data, model, metric_list=metric_list):\n",
    "    y_real = eval_data.labels.values.tolist()\n",
    "    print('Tokenizing docs...')\n",
    "    enc_eval_data = tokenize_docs(eval_data.text.values, model.tokenizer)\n",
    "    y_pred = model.get_predictions(enc_eval_data)\n",
    "    results={}\n",
    "    print('Getting metrics:')\n",
    "    for metric in metric_list:\n",
    "        if metric=='accuracy':\n",
    "            m = load_metric(metric)\n",
    "        else:\n",
    "            m = load_metric(metric, 'macro')\n",
    "        results[metric] = m.compute(predictions=y_pred, references=y_real)\n",
    "        print(f'{metric}: {results[metric]}')\n",
    "    print(f'Confusion matrix:\\n {confusion_matrix(y_real, y_pred)}')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a3e67",
   "metadata": {},
   "source": [
    "### DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cd837e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating PoliticFact dataset:\n",
      "\n",
      "With model trained with 2000 from Experiment 1\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a94195050e463a90e2ed2eed8c80bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba14a68c87ee49e291ee82f572273975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp/ipykernel_4456/3611526556.py:55: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  m = load_metric(metric)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: {'accuracy': 0.7518939393939394}\n",
      "f1: {'f1': 0.7095343680709535}\n",
      "precision: {'precision': 0.6808510638297872}\n",
      "recall: {'recall': 0.7407407407407407}\n",
      "Confusion matrix:\n",
      " [[474 150]\n",
      " [112 320]]\n",
      "\n",
      "With model trained with 5000 from Experiment 1\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93eb798d06f8426c8d020515c37b866a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e02afd165f643ecb91c1864e1f53bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.7585227272727273}\n",
      "f1: {'f1': 0.7098976109215017}\n",
      "precision: {'precision': 0.697986577181208}\n",
      "recall: {'recall': 0.7222222222222222}\n",
      "Confusion matrix:\n",
      " [[489 135]\n",
      " [120 312]]\n",
      "\n",
      "With model from Experiment 2\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53ebeee47af4c11af959e6f9646ff2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888147c5e740432d8e3ade06fa73972b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.4090909090909091}\n",
      "f1: {'f1': 0.5806451612903226}\n",
      "precision: {'precision': 0.4090909090909091}\n",
      "recall: {'recall': 1.0}\n",
      "Confusion matrix:\n",
      " [[  0 624]\n",
      " [  0 432]]\n",
      "Evaluating GossipCop dataset:\n",
      "\n",
      "With model trained with 2000 from Experiment 1\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668067d764604777a2f195cfcaf2061b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f65585dbb347e5b9f0506eba42d788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.3614724480578139}\n",
      "f1: {'f1': 0.3856950419328206}\n",
      "precision: {'precision': 0.25087620124364046}\n",
      "recall: {'recall': 0.8337403719706932}\n",
      "Confusion matrix:\n",
      " [[ 3565 13252]\n",
      " [  885  4438]]\n",
      "\n",
      "With model trained with 5000 from Experiment 1\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02fe3edf912244a1a5871e88ed4240f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53e7965e7a1469b92c26b22d87ff2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.4322041553748871}\n",
      "f1: {'f1': 0.3852511125238398}\n",
      "precision: {'precision': 0.26041253470844905}\n",
      "recall: {'recall': 0.7399962427202705}\n",
      "Confusion matrix:\n",
      " [[ 5630 11187]\n",
      " [ 1384  3939]]\n",
      "\n",
      "With model from Experiment 2\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0000f2daac481a92d54897b8db0a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e342718bf644aada2e856ad5883b955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.24046973803071364}\n",
      "f1: {'f1': 0.3876629524433763}\n",
      "precision: {'precision': 0.24043543068792628}\n",
      "recall: {'recall': 1.0}\n",
      "Confusion matrix:\n",
      " [[    1 16816]\n",
      " [    0  5323]]\n",
      "Evaluating LIAR dataset:\n",
      "\n",
      "With model trained with 2000 from Experiment 1\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5602229f9054e6bbd3d424c653bbc46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae94430cf4ca4f008080fabc949beaf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.5013157894736842}\n",
      "f1: {'f1': 0.5028421512898994}\n",
      "precision: {'precision': 0.5563618771165941}\n",
      "recall: {'recall': 0.45871559633027525}\n",
      "Confusion matrix:\n",
      " [[1136  917]\n",
      " [1357 1150]]\n",
      "\n",
      "With model trained with 5000 from Experiment 1\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d84c55efd54b7e89d34941b74bc1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdfc1ffe5ea469a9c79ba4adab80f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.5070175438596491}\n",
      "f1: {'f1': 0.5978533094812165}\n",
      "precision: {'precision': 0.5420045410314629}\n",
      "recall: {'recall': 0.6665337056242521}\n",
      "Confusion matrix:\n",
      " [[ 641 1412]\n",
      " [ 836 1671]]\n",
      "\n",
      "With model from Experiment 2\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80dcb6ce7e47459f83bb241efee7bd9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c3eecb428241e79c5ae9c7899650ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.55}\n",
      "f1: {'f1': 0.7094307561597281}\n",
      "precision: {'precision': 0.5499451152579583}\n",
      "recall: {'recall': 0.9992022337455125}\n",
      "Confusion matrix:\n",
      " [[   3 2050]\n",
      " [   2 2505]]\n"
     ]
    }
   ],
   "source": [
    "#Load models\n",
    "model1_2 = Model(input_dir_1_2, NUM_LABELS) #EXP1\n",
    "model1_5 = Model(input_dir_1_5, NUM_LABELS) \n",
    "model2 = Model(input_dir_2, NUM_LABELS) #EXP2\n",
    "\n",
    "print('Evaluating PoliticFact dataset:')\n",
    "print('\\nWith model trained with 2000 from Experiment 1')\n",
    "m11p = get_metrics(politic_data, model1_2)\n",
    "print('\\nWith model trained with 5000 from Experiment 1')\n",
    "m12p = get_metrics(politic_data, model1_5)\n",
    "print('\\nWith model from Experiment 2')\n",
    "m13p = get_metrics(politic_data, model2)\n",
    "\n",
    "print('Evaluating GossipCop dataset:')\n",
    "print('\\nWith model trained with 2000 from Experiment 1')\n",
    "m11g = get_metrics(gossip_data, model1_2)\n",
    "print('\\nWith model trained with 5000 from Experiment 1')\n",
    "m12g = get_metrics(gossip_data, model1_5)\n",
    "print('\\nWith model from Experiment 2')\n",
    "m13g = get_metrics(gossip_data, model2)\n",
    "\n",
    "print('Evaluating LIAR dataset:')\n",
    "print('\\nWith model trained with 2000 from Experiment 1')\n",
    "m11l = get_metrics(liar_data, model1_2)\n",
    "print('\\nWith model trained with 5000 from Experiment 1')\n",
    "m12l = get_metrics(liar_data, model1_5)\n",
    "print('\\nWith model from Experiment 2')\n",
    "m13l = get_metrics(liar_data, model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9231f",
   "metadata": {},
   "source": [
    "### BERT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be5e2495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating PoliticFact dataset:\n",
      "\n",
      "With model trained with 2000 from Experiment 1\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e4fcc707ad44a6bf26dd29c14a901c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b388bcf99142e8b725f76abff61294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.7433712121212122}\n",
      "f1: {'f1': 0.6972067039106145}\n",
      "precision: {'precision': 0.673866090712743}\n",
      "recall: {'recall': 0.7222222222222222}\n",
      "Confusion matrix:\n",
      " [[473 151]\n",
      " [120 312]]\n",
      "\n",
      "With model trained with 5000 from Experiment 1\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b34f68c7e3e4f5aaf3810d89c77ad65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83572c0af16348beb0625d96f6bb1441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.7339015151515151}\n",
      "f1: {'f1': 0.6895027624309392}\n",
      "precision: {'precision': 0.6596194503171248}\n",
      "recall: {'recall': 0.7222222222222222}\n",
      "Confusion matrix:\n",
      " [[463 161]\n",
      " [120 312]]\n",
      "\n",
      "With model from Experiment 2\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c408567a014dc6bb97fe13ddf66057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f659577a3d4995a26b007db06a74e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.4109848484848485}\n",
      "f1: {'f1': 0.5814266487213997}\n",
      "precision: {'precision': 0.4098671726755218}\n",
      "recall: {'recall': 1.0}\n",
      "Confusion matrix:\n",
      " [[  2 622]\n",
      " [  0 432]]\n",
      "Evaluating GossipCop dataset:\n",
      "\n",
      "With model trained with 2000 from Experiment 1\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e29269e1074e2ba0a65470df56ac26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9b9b19be3a4649a65fbfaf1dcbaf2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.35}\n",
      "f1: {'f1': 0.37817050512033873}\n",
      "precision: {'precision': 0.24556677890011222}\n",
      "recall: {'recall': 0.822092804809318}\n",
      "Confusion matrix:\n",
      " [[ 3373 13444]\n",
      " [  947  4376]]\n",
      "\n",
      "With model trained with 5000 from Experiment 1\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae951f4ac054e8c983f850069aae612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88eeffe1182f4330b835c367a1fe6f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.394579945799458}\n",
      "f1: {'f1': 0.37574515648286144}\n",
      "precision: {'precision': 0.2497987491485541}\n",
      "recall: {'recall': 0.7578433214352809}\n",
      "Confusion matrix:\n",
      " [[ 4702 12115]\n",
      " [ 1289  4034]]\n",
      "\n",
      "With model from Experiment 2\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e6c4d135ba429886148229d7438ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4423a56a7b044b5b839fdd146c7be602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.2406955736224029}\n",
      "f1: {'f1': 0.38768894554725913}\n",
      "precision: {'precision': 0.24046629315018977}\n",
      "recall: {'recall': 0.9998121360135263}\n",
      "Confusion matrix:\n",
      " [[    7 16810]\n",
      " [    1  5322]]\n",
      "Evaluating LIAR dataset:\n",
      "\n",
      "With model trained with 2000 from Experiment 1\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06491457c4e946c391396b639e6d4e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f95ab12b62441296b421943cc22cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.5131578947368421}\n",
      "f1: {'f1': 0.580973952434881}\n",
      "precision: {'precision': 0.5514152633464708}\n",
      "recall: {'recall': 0.6138811328280813}\n",
      "Confusion matrix:\n",
      " [[ 801 1252]\n",
      " [ 968 1539]]\n",
      "\n",
      "With model trained with 5000 from Experiment 1\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcc1ef35ec84eafbd4a99edd6237644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39909f7ccf14011a288af0dd2d3eb21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.49890350877192985}\n",
      "f1: {'f1': 0.6156433978132885}\n",
      "precision: {'precision': 0.5322862129144852}\n",
      "recall: {'recall': 0.7299561228560032}\n",
      "Confusion matrix:\n",
      " [[ 445 1608]\n",
      " [ 677 1830]]\n",
      "\n",
      "With model from Experiment 2\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807c39539f254e63afd3fbd0301cbd79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417a5eff7117406a934acc2587ffb401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.5486842105263158}\n",
      "f1: {'f1': 0.7077534791252483}\n",
      "precision: {'precision': 0.5495038588754134}\n",
      "recall: {'recall': 0.9940167530913442}\n",
      "Confusion matrix:\n",
      " [[  10 2043]\n",
      " [  15 2492]]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'BERT'\n",
    "input_dir_1_2 = os.path.join(os.getcwd(), f'models/{model_name}/EXP_1/2000') \n",
    "input_dir_1_5 = os.path.join(os.getcwd(), f'models/{model_name}/EXP_1/5000') \n",
    "input_dir_2 = os.path.join(os.getcwd(), f'models/{model_name}/EXP_2') \n",
    "output_dir = os.path.join(os.getcwd(), f'models/{model_name}/EXP_{experiment}')\n",
    "\n",
    "model1_2 = Model(input_dir_1_2, NUM_LABELS) #EXP1\n",
    "model1_5 = Model(input_dir_1_5, NUM_LABELS) \n",
    "model2 = Model(input_dir_2, NUM_LABELS) #EXP2\n",
    "\n",
    "print('Evaluating PoliticFact dataset:')\n",
    "print('\\nWith model trained with 2000 from Experiment 1')\n",
    "m21p = get_metrics(politic_data, model1_2)\n",
    "print('\\nWith model trained with 5000 from Experiment 1')\n",
    "m22p = get_metrics(politic_data, model1_5)\n",
    "print('\\nWith model from Experiment 2')\n",
    "m23p = get_metrics(politic_data, model2)\n",
    "\n",
    "print('Evaluating GossipCop dataset:')\n",
    "print('\\nWith model trained with 2000 from Experiment 1')\n",
    "m21g = get_metrics(gossip_data, model1_2)\n",
    "print('\\nWith model trained with 5000 from Experiment 1')\n",
    "m22g = get_metrics(gossip_data, model1_5)\n",
    "print('\\nWith model from Experiment 2')\n",
    "m23g = get_metrics(gossip_data, model2)\n",
    "\n",
    "print('Evaluating LIAR dataset:')\n",
    "print('\\nWith model trained with 2000 from Experiment 1')\n",
    "m11l = get_metrics(liar_data, model1_2)\n",
    "print('\\nWith model trained with 5000 from Experiment 1')\n",
    "m12l = get_metrics(liar_data, model1_5)\n",
    "print('\\nWith model from Experiment 2')\n",
    "m13l = get_metrics(liar_data, model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d138608",
   "metadata": {},
   "source": [
    "### RoBERTa Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5a1a3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating PoliticFact dataset:\n",
      "\n",
      "With model trained with 2000 from Experiment 1\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb636aa25d74b198db029b642795c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75dc9a0717a342aa8c7850f1d2c0438a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.6950757575757576}\n",
      "f1: {'f1': 0.6652806652806652}\n",
      "precision: {'precision': 0.6037735849056604}\n",
      "recall: {'recall': 0.7407407407407407}\n",
      "Confusion matrix:\n",
      " [[414 210]\n",
      " [112 320]]\n",
      "\n",
      "With model trained with 5000 from Experiment 1\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed14d9d0ca3d43609ed2eecdd77d2c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc8b2a856b04fcc9f3d78fc2b696e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.7054924242424242}\n",
      "f1: {'f1': 0.6702014846235418}\n",
      "precision: {'precision': 0.6183953033268101}\n",
      "recall: {'recall': 0.7314814814814815}\n",
      "Confusion matrix:\n",
      " [[429 195]\n",
      " [116 316]]\n",
      "\n",
      "With model from Experiment 2\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef22400d5d61487ba944d8fb826cc580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6afc6830ae174ae08cbd94e3e2893ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.4100378787878788}\n",
      "f1: {'f1': 0.5810356422326832}\n",
      "precision: {'precision': 0.409478672985782}\n",
      "recall: {'recall': 1.0}\n",
      "Confusion matrix:\n",
      " [[  1 623]\n",
      " [  0 432]]\n",
      "Evaluating GossipCop dataset:\n",
      "\n",
      "With model trained with 2000 from Experiment 1\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6457e6fd5f1e4e02a2214df7e6ff2f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76d3587e386447b8ccdd6a7a30d9da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.3775519421860885}\n",
      "f1: {'f1': 0.3732775478648416}\n",
      "precision: {'precision': 0.24624984999399976}\n",
      "recall: {'recall': 0.7709938004884463}\n",
      "Confusion matrix:\n",
      " [[ 4255 12562]\n",
      " [ 1219  4104]]\n",
      "\n",
      "With model trained with 5000 from Experiment 1\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e7713122cd4af183f6943101a381e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff94b09cbcf74caf8f6e155efd34312c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.4399728997289973}\n",
      "f1: {'f1': 0.37051327613342133}\n",
      "precision: {'precision': 0.2538611381661333}\n",
      "recall: {'recall': 0.6855156866428705}\n",
      "Confusion matrix:\n",
      " [[ 6092 10725]\n",
      " [ 1674  3649]]\n",
      "\n",
      "With model from Experiment 2\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a4e726997e474b876d988a21c3362e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7226813014e48b4a395b416b9519aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.24056007226738935}\n",
      "f1: {'f1': 0.3876465875154782}\n",
      "precision: {'precision': 0.24043370228145472}\n",
      "recall: {'recall': 0.9998121360135263}\n",
      "Confusion matrix:\n",
      " [[    4 16813]\n",
      " [    1  5322]]\n",
      "Evaluating LIAR dataset:\n",
      "\n",
      "With model trained with 2000 from Experiment 1\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022c9435030a41ae83097c3ee3d8d905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0ea063181e4effa958ad9a2542d64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.5287280701754385}\n",
      "f1: {'f1': 0.6329632792485055}\n",
      "precision: {'precision': 0.5534647550776584}\n",
      "recall: {'recall': 0.7391304347826086}\n",
      "Confusion matrix:\n",
      " [[ 558 1495]\n",
      " [ 654 1853]]\n",
      "\n",
      "With model trained with 5000 from Experiment 1\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607956d6716c4482acd43e36169dd6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf7b76454a4467daad77d83ba774981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.5252192982456141}\n",
      "f1: {'f1': 0.6732075471698112}\n",
      "precision: {'precision': 0.5415250121418164}\n",
      "recall: {'recall': 0.8895093737534903}\n",
      "Confusion matrix:\n",
      " [[ 165 1888]\n",
      " [ 277 2230]]\n",
      "\n",
      "With model from Experiment 2\n",
      "Tokenizing docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5e34664abd4e67b73de2e41e116485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb9a4d234e543b0a4613b1909fc7639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics:\n",
      "accuracy: {'accuracy': 0.5510964912280701}\n",
      "f1: {'f1': 0.7099333994615276}\n",
      "precision: {'precision': 0.5505494505494506}\n",
      "recall: {'recall': 0.9992022337455125}\n",
      "Confusion matrix:\n",
      " [[   8 2045]\n",
      " [   2 2505]]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'RoBERTa'\n",
    "input_dir_1_2 = os.path.join(os.getcwd(), f'models/{model_name}/EXP_1/2000') \n",
    "input_dir_1_5 = os.path.join(os.getcwd(), f'models/{model_name}/EXP_1/5000') \n",
    "input_dir_2 = os.path.join(os.getcwd(), f'models/{model_name}/EXP_2') \n",
    "output_dir = os.path.join(os.getcwd(), f'models/{model_name}/EXP_{experiment}')\n",
    "\n",
    "model1_2 = Model(input_dir_1_2, NUM_LABELS) #EXP1\n",
    "model1_5 = Model(input_dir_1_5, NUM_LABELS) \n",
    "model2 = Model(input_dir_2, NUM_LABELS) #EXP2\n",
    "\n",
    "print('Evaluating PoliticFact dataset:')\n",
    "print('\\nWith model trained with 2000 from Experiment 1')\n",
    "m21p = get_metrics(politic_data, model1_2)\n",
    "print('\\nWith model trained with 5000 from Experiment 1')\n",
    "m22p = get_metrics(politic_data, model1_5)\n",
    "print('\\nWith model from Experiment 2')\n",
    "m23p = get_metrics(politic_data, model2)\n",
    "\n",
    "print('Evaluating GossipCop dataset:')\n",
    "print('\\nWith model trained with 2000 from Experiment 1')\n",
    "m21g = get_metrics(gossip_data, model1_2)\n",
    "print('\\nWith model trained with 5000 from Experiment 1')\n",
    "m22g = get_metrics(gossip_data, model1_5)\n",
    "print('\\nWith model from Experiment 2')\n",
    "m23g = get_metrics(gossip_data, model2)\n",
    "\n",
    "print('Evaluating LIAR dataset:')\n",
    "print('\\nWith model trained with 2000 from Experiment 1')\n",
    "m11l = get_metrics(liar_data, model1_2)\n",
    "print('\\nWith model trained with 5000 from Experiment 1')\n",
    "m12l = get_metrics(liar_data, model1_5)\n",
    "print('\\nWith model from Experiment 2')\n",
    "m13l = get_metrics(liar_data, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a20471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
